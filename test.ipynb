{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def split_text_with_st_splitter(text, max_tokens=500):\n",
    "    # Initialize text splitter for token-based chunking\n",
    "    splitter = SentenceTransformersTokenTextSplitter.from_tiktoken_encoder(\n",
    "        encoding_name=\"cl100k_base\",  # This tokenizer works for OpenAI models\n",
    "        chunk_size=max_tokens,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"The evolution of digital technologies has changed the way works and other protected subject-matter are created, produced, distributed and exploited. New uses have emerged as well as new actors and new business models. In the digital environment, cross-border uses have also intensified and new opportunities for consumers to access copyright-protected content have materialised. Even though the objectives and principles laid down by the EU copyright framework remain sound, there is a need to adapt it to these new realities. Intervention at EU level is also needed to avoid fragmentation in the internal market. Against this background, the Digital Single Market Strategy 1 adopted in May 2015 identified the need “to reduce the differences between national copyright regimes and allow for wider online access to works by users across the EU”. This Communication highlighted the importance to enhance cross-border access to copyright-protected content services, facilitate new uses in the fields of research and education, and clarify the role of online services in the distribution of works and other subject-matter. In December 2015, the Commission issued a Communication ‘Towards a modern, more European copyright framework’ 2 . This Communication outlined targeted actions and a long-term vision to modernise EU copyright rules. This proposal is one of the measures aiming at addressing specific issues identified in that Communication.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformer python package. This is needed in order to for SentenceTransformersTokenTextSplitter. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa_practical/lib/python3.13/site-packages/langchain_text_splitters/sentence_transformers.py:22\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[0;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplit_text_with_st_splitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36msplit_text_with_st_splitter\u001b[0;34m(text, max_tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text_with_st_splitter\u001b[39m(text, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize text splitter for token-based chunking\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     splitter \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformersTokenTextSplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tiktoken_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcl100k_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This tokenizer works for OpenAI models\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m splitter\u001b[38;5;241m.\u001b[39msplit_text(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa_practical/lib/python3.13/site-packages/langchain_text_splitters/base.py:212\u001b[0m, in \u001b[0;36mTextSplitter.from_tiktoken_encoder\u001b[0;34m(cls, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     extra_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding_name,\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed_special\u001b[39m\u001b[38;5;124m\"\u001b[39m: allowed_special,\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisallowed_special\u001b[39m\u001b[38;5;124m\"\u001b[39m: disallowed_special,\n\u001b[1;32m    209\u001b[0m     }\n\u001b[1;32m    210\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs}\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlength_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tiktoken_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/qa_practical/lib/python3.13/site-packages/langchain_text_splitters/sentence_transformers.py:24\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[0;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformer python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to for SentenceTransformersTokenTextSplitter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import sentence_transformer python package. This is needed in order to for SentenceTransformersTokenTextSplitter. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "chunks = split_text_with_st_splitter(document)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
