{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from utils import create_opensearch_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reindex response: {'took': 16740, 'timed_out': False, 'total': 14914, 'updated': 0, 'created': 14914, 'deleted': 0, 'batches': 15, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []}\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "opensearch_user = os.getenv('OPENSEARCH_USER')\n",
    "opensearch_password = os.getenv('OPENSEARCH_PASSWORD')\n",
    "\n",
    "# Connect to OpenSearch\n",
    "client = create_opensearch_client(username=opensearch_user, password=opensearch_password)\n",
    "embedding_dim =  1536\n",
    "\n",
    "# Define the new index mapping with knn vector search settings\n",
    "new_index_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True \n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"url\": {\"type\": \"keyword\"},\n",
    "            \"chunk_id\": {\"type\": \"integer\"},\n",
    "            \"text\": {\"type\": \"text\", \"fielddata\": False},\n",
    "            \"embedding\": {\"type\": \"knn_vector\", \"dimension\": embedding_dim}  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the new index\n",
    "new_index_name = \"eur-lex-diversified-knowledge-base-3\"\n",
    "old_index_name = \"eur-lex-diversified-knowledge-base-2\"\n",
    "client.indices.create(index=new_index_name, body=new_index_mapping, ignore=400)\n",
    "\n",
    "# Define the reindex request\n",
    "reindex_body = {\n",
    "    \"source\": {\n",
    "        \"index\": old_index_name\n",
    "    },\n",
    "    \"dest\": {\n",
    "        \"index\": new_index_name\n",
    "    }\n",
    "}\n",
    "\n",
    "# Reindex the documents from the old index to the new one\n",
    "response = client.reindex(body=reindex_body)\n",
    "print(\"Reindex response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the new index 'eur-lex-diversified-knowledge-base-3': 14914\n"
     ]
    }
   ],
   "source": [
    "response = client.count(index=new_index_name)\n",
    "\n",
    "# Extract and print the document count\n",
    "doc_count = response['count']\n",
    "print(f\"Number of documents in the new index '{new_index_name}': {doc_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that similarity search works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from utils import create_opensearch_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "opensearch_user = os.getenv('OPENSEARCH_USER')\n",
    "opensearch_password = os.getenv('OPENSEARCH_PASSWORD')\n",
    "opensearch_client = create_opensearch_client(username=opensearch_user, password=opensearch_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions_and_answers_from_opensearch(qa_index_name, opensearch_client, size=1):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    response = opensearch_client.search(index=qa_index_name, body=query)\n",
    "\n",
    "    questions_and_answers = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        question = hit['_source']['question']\n",
    "        answer = hit['_source']['answer']\n",
    "        questions_and_answers.append((question, answer))\n",
    "    \n",
    "    return questions_and_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key= openai_api_key, model=\"text-embedding-3-small\")\n",
    "opensearch_url = \"https://opensearch-ds-2.ifi.uni-heidelberg.de:443\"\n",
    "qa_index_name = \"eur-lex-diversified-qa-askep\"\n",
    "k = 1\n",
    "\n",
    "    \n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    index_name=new_index_name, \n",
    "    embedding_function=embeddings,\n",
    "    vector_field=\"embedding\",\n",
    "    opensearch_url=opensearch_url,\n",
    "    http_auth=(opensearch_user, opensearch_password),\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "questions_and_answers = load_questions_and_answers_from_opensearch(qa_index_name, opensearch_client, size=1)\n",
    "for question, ground_truth_answer in questions_and_answers:\n",
    "        docs = vector_store.similarity_search(query=question, k=k, vector_field=\"embedding\")\n",
    "        for doc in docs:\n",
    "            print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_community.vectorstores.utils import maximal_marginal_relevance\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_marginal_relevance_search(\n",
    "        vector_store: VectorStore,\n",
    "        query: str,\n",
    "        k: int = 4,\n",
    "        fetch_k: int = 20,\n",
    "        lambda_mult: float = 0.5,\n",
    "        **kwargs: Any,\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"Return docs selected using the maximal marginal relevance.\n",
    "\n",
    "        Maximal marginal relevance optimizes for similarity to query AND diversity\n",
    "        among selected documents.\n",
    "\n",
    "        Args:\n",
    "            query: Text to look up documents similar to.\n",
    "            k: Number of Documents to return. Defaults to 4.\n",
    "            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
    "                     Defaults to 20.\n",
    "            lambda_mult: Number between 0 and 1 that determines the degree\n",
    "                        of diversity among the results with 0 corresponding\n",
    "                        to maximum diversity and 1 to minimum diversity.\n",
    "                        Defaults to 0.5.\n",
    "        Returns:\n",
    "            List of Documents selected by maximal marginal relevance.\n",
    "        \"\"\"\n",
    "\n",
    "        vector_field = kwargs.get(\"vector_field\", \"vector_field\")\n",
    "        text_field = kwargs.get(\"text_field\", \"text\")\n",
    "        metadata_field = kwargs.get(\"metadata_field\", \"metadata\")\n",
    "\n",
    "        # Get embedding of the user query\n",
    "        embedding = vector_store.embedding_function.embed_query(query)\n",
    "\n",
    "        # Do ANN/KNN search to get top fetch_k results where fetch_k >= k\n",
    "        results = vector_store._raw_similarity_search_with_score_by_vector(\n",
    "            embedding, fetch_k, **kwargs\n",
    "        )\n",
    "\n",
    "        embeddings = [result[\"_source\"][vector_field] for result in results]\n",
    "\n",
    "        # Rerank top k results using MMR, (mmr_selected is a list of indices)\n",
    "        mmr_selected = maximal_marginal_relevance(\n",
    "            np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult\n",
    "        )\n",
    "        print(len(mmr_selected))\n",
    "\n",
    "        return [\n",
    "            Document(\n",
    "                page_content=results[i][\"_source\"][text_field],\n",
    "                id=results[i][\"_id\"],\n",
    "            )\n",
    "            for i in mmr_selected\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k, \"vector_field\": \"embedding\"})\n",
    "mmr_retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\n",
    "    'k': 6, \n",
    "    'lambda_mult': 0.5, \n",
    "    \"vector_field\": \"embedding\", \n",
    "    \"metadata_field\": None,\n",
    "})\n",
    "for question, ground_truth_answer in questions_and_answers:\n",
    "    retrieved_docs = mmr_retriever.invoke(question)\n",
    "    for doc in retrieved_docs:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "for question, ground_truth_answer in questions_and_answers:\n",
    "    docs = max_marginal_relevance_search(vector_store, query=question, k=k, vector_field=\"embedding\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Doc nr: {i+1}\\n\\n\")\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key= openai_api_key, model=\"text-embedding-3-small\")\n",
    "opensearch_url = \"https://opensearch-ds-2.ifi.uni-heidelberg.de:443\"\n",
    "qa_index_name = \"eur-lex-diversified-qa-askep\"\n",
    "kb_index_name = \"eur-lex-diversified-knowledge-base-3\"\n",
    "k = 2\n",
    "\n",
    "    \n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    index_name=kb_index_name, \n",
    "    embedding_function=embeddings,\n",
    "    vector_field=\"embedding\",\n",
    "    opensearch_url=opensearch_url,\n",
    "    http_auth=(opensearch_user, opensearch_password),\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k, \"vector_field\": \"embedding\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    The following documents provide relevant information: {context}\n",
    "    Please answer the question only by using the provided information. Make sure to provide a diversified response that covers different perspectives and details from the provided documents. Your answer should include multiple viewpoints and insights from the context, not just a single perspective. If necessary, highlight different interpretations, opinions, or additional context that is relevant to the question.\n",
    "    Answer the question comprehensively, using the information from the documents provided.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key = openai_api_key)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "questions_and_answers = load_questions_and_answers_from_opensearch(qa_index_name, opensearch_client, size=1)\n",
    "for i, (question, ground_truth_answer) in enumerate(questions_and_answers):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Ground truth answer: {ground_truth_answer}\")\n",
    "    print(\"-\"*50)\n",
    "    response = qa_chain({\"query\": question})\n",
    "    # print(\"Retrieved Documents:\")\n",
    "    # for doc in response[\"source_documents\"]:\n",
    "    #     print(f\"- {doc.page_content}\")\n",
    "    #     print(\"-\" * 50)\n",
    "    print(response[\"result\"])\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
