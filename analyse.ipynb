{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove all duplicate urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate URLs removed. Cleaned list saved to cleaned_urls.json\n",
      "There are 2158 unique URLs in the list.\n"
     ]
    }
   ],
   "source": [
    "# Input file with URLs\n",
    "input_file = \"all_urls.json\"\n",
    "output_file = \"cleaned_urls.json\"\n",
    "\n",
    "# Load the list of URLs\n",
    "with open(input_file, \"r\") as file:\n",
    "    urls = json.load(file)\n",
    "\n",
    "# Remove duplicates by converting to a set and back to a list\n",
    "unique_urls = list(set(urls))\n",
    "\n",
    "# Sort the URLs for consistency\n",
    "unique_urls.sort()\n",
    "\n",
    "# Save the cleaned list to a new JSON file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(unique_urls, file, indent=4)\n",
    "\n",
    "print(f\"Duplicate URLs removed. Cleaned list saved to {output_file}\")\n",
    "print(f\"There are {len(unique_urls)} unique URLs in the list.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate youtube urls, urls that cannot be accessed, urls of pages that cannot be found or insecure urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"cleaned_urls.json\"\n",
    "output_file = \"cleaned_urls.json\"\n",
    "\n",
    "# Load the list of URLs\n",
    "with open(input_file, \"r\") as file:\n",
    "    urls = json.load(file)\n",
    "\n",
    "unwanted_urls = [\"https://youtu.be/Qlzyp6QiUyA\",\n",
    "                 \"http://eacea.ec.europa.eu/bilateral_cooperation/index_en.php\",\n",
    "                 \"http://eacea.ec.europa.eu/erasmus_mundus/\",\n",
    "                 \"http://eacea.ec.europa.eu/tempus/\",\n",
    "                 \"http://www.elections2014.eu/en/\",\n",
    "                 \"http://www.elections2014.eu/en/in-the-member-states\",\n",
    "                 \"http://www.elections2014.eu/en/in-the-member-states/european-union\",\n",
    "                 \"http://www.elections2014.eu/en/new-commission/hearings/by-committee\",\n",
    "                 \"http://www.elections2014.eu/en/new-commission/portfolios-and-candidates\",\n",
    "                 \"http://www.elections2014.eu/en/new-parliament\",\n",
    "                 \"http://www.elections2014.eu/en/news-room/content/20140918IFG65303/html/Infographic-how-the-European-Commission-will-get-elected\",\n",
    "                 \"http://www.elections2014.eu/en/press-kit/content/20131112PKH24411/html/Overview-of-Parliament-and-the-2014-elections\",\n",
    "                 \"https://europa.eu/eyd2015/\",\n",
    "                 \"https://www.avrupa.info.tr/en\"]\n",
    "new_urls = [url for url in urls if url not in unwanted_urls]\n",
    "with open(output_file, 'w') as file:\n",
    "        json.dump(new_urls, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs: 2144\n",
      "Sorted prefix counts saved to prefix_counts.json\n"
     ]
    }
   ],
   "source": [
    "# Input file with URLs\n",
    "input_file = \"cleaned_urls.json\"\n",
    "output_file = \"prefix_counts.json\"\n",
    "\n",
    "# Load the list of URLs\n",
    "with open(input_file, \"r\") as file:\n",
    "    urls = json.load(file)\n",
    "\n",
    "# Extract prefixes and count their occurrences\n",
    "prefix_counts = {}\n",
    "for url in urls:\n",
    "    # Parse the URL to extract the scheme and netloc\n",
    "    parsed_url = urlparse(url)\n",
    "    prefix = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "    prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1\n",
    "\n",
    "# Calculate the total count\n",
    "total_count = sum(prefix_counts.values())\n",
    "\n",
    "# Sort prefixes by counts in descending order\n",
    "sorted_prefix_counts = sorted(prefix_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Convert to a dictionary for saving (optional, as JSON supports list of tuples)\n",
    "sorted_prefix_counts_dict = {k: v for k, v in sorted_prefix_counts}\n",
    "\n",
    "# Save the sorted counts to a JSON file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(sorted_prefix_counts_dict, file, indent=4)\n",
    "\n",
    "# Print the total count and save confirmation\n",
    "print(f\"Total number of URLs: {total_count}\")\n",
    "print(f\"Sorted prefix counts saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All URLs start with 'http'.\n"
     ]
    }
   ],
   "source": [
    "# Input file with URLs\n",
    "input_file = \"cleaned_urls.json\"\n",
    "\n",
    "# Load the list of URLs\n",
    "with open(input_file, \"r\") as file:\n",
    "    urls = json.load(file)\n",
    "\n",
    "# Filter and print URLs that do not start with \"http\"\n",
    "invalid_urls = [url for url in urls if not url.startswith(\"http\")]\n",
    "\n",
    "# Print the invalid URLs\n",
    "if invalid_urls:\n",
    "    print(\"URLs that do not start with 'http':\")\n",
    "    for url in invalid_urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"All URLs start with 'http'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load URLs from a file\n",
    "def load_urls(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# save URLs to a file\n",
    "def save_urls(file_path, urls):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(urls, file, indent=4)\n",
    "\n",
    "def is_pdf(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=10, allow_redirects=True)\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "\n",
    "        # Debug: Check headers\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        print(f\"URL: {url} | Content-Type: {content_type}\")\n",
    "\n",
    "        # Check for PDF MIME type in headers\n",
    "        if 'application/pdf' in content_type:\n",
    "            return True\n",
    "\n",
    "        # Read magic number from the first few bytes\n",
    "        first_bytes = response.raw.read(1024)  # Read the first 1KB for better accuracy\n",
    "        if first_bytes.startswith(b'%PDF'):\n",
    "            return True\n",
    "\n",
    "        return False  # Not a PDF if neither MIME nor magic number matches\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Debug: Log HTTP errors\n",
    "        print(f\"HTTP error for URL: {url} | Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # Debug: Log other errors\n",
    "        print(f\"Error for URL: {url} | Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'cleaned_urls.json'\n",
    "pdf_file = 'pdf_urls.json'\n",
    "nonpdf_file = 'nonpdf_urls.json'\n",
    "error_file = 'error_urls.json'\n",
    "\n",
    "# Load the URLs\n",
    "urls = load_urls(input_file)\n",
    "\n",
    "# Initialize lists for categorization\n",
    "pdf_urls = []\n",
    "nonpdf_urls = []\n",
    "error_urls = []\n",
    "\n",
    "# Iterate through URLs and categorize\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        if is_pdf(url):\n",
    "            pdf_urls.append(url)\n",
    "        else:\n",
    "            nonpdf_urls.append(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_urls.append({\"url\": url, \"error\": str(e)})\n",
    "    except Exception as e:\n",
    "        error_urls.append({\"url\": url, \"error\": str(e)})\n",
    "\n",
    "# Save categorized URLs\n",
    "save_urls(pdf_file, pdf_urls)\n",
    "save_urls(nonpdf_file, nonpdf_urls)\n",
    "save_urls(error_file, error_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
